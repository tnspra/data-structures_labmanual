{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnspra/data-structures_labmanual/blob/main/PCA_vs_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA and LDA\n",
        "\n",
        "Exam Registration Number:24011103063\n",
        "\n",
        "Name:Tata Naga Sripranav\n",
        "\n",
        "\n",
        "\n",
        "![PCA vs LDA](https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png)"
      ],
      "metadata": {
        "id": "1FwyMRGjouRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA on Images\n",
        "Perform PCA on image data and study reconstruction quality for\n",
        "\n",
        "$$ k=2,4,8,16,64,256 $$\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "- Select one image randomly\n",
        "\n",
        "- Convert image into a vector of size $ 4096 \\times 1$\n",
        "\n",
        "- Normalize pixel values to $[0,1]$\n",
        "\n",
        "- Find the Covariance Matrix and Perform PCA.\n",
        "\n",
        "- Code for SCREE PLOT (Check for Elbow shape)\n",
        "\n",
        "- Reconstruct the image (Figure out)\n",
        "\n",
        "- Display all reconstructions in a grid and relate it with SCREE Plot\n",
        "\n",
        "You will use olivetti faces dataset. Let $n$ be the total number of images in the dataset. let $x$ be the original image and $x'$ be the image reconstructed using $k$ principal components then its MSE is given as follows:\n",
        "\n",
        "$$MSE=\\frac{1}{n}||xâˆ’x'||^2$$\n",
        "\n",
        "- Plot MSE vs k"
      ],
      "metadata": {
        "id": "H2H0YLOvqAod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load and Preprocess\n",
        "# fetch_olivetti_faces automatically normalizes pixel values to [0, 1]\n",
        "faces_data = fetch_olivetti_faces()\n",
        "X = faces_data.data  # shape (400, 4096)\n",
        "\n",
        "# Select one image randomly for reconstruction\n",
        "random_idx = np.random.randint(0, 400)\n",
        "original_face = X[random_idx]\n",
        "\n",
        "# 2. Perform PCA and calculate Scree Plot data\n",
        "# We fit on the full 4096 components initially to see the whole variance\n",
        "pca_full = PCA().fit(X)\n",
        "explained_variance = pca_full.explained_variance_ratio_\n",
        "\n",
        "# 3. Study Reconstruction Quality for specific k values\n",
        "k_values = [2, 4, 8, 16, 64, 256]\n",
        "reconstructions = []\n",
        "mse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    pca = PCA(n_components=k)\n",
        "    # Step: Transform (Compress) and then Inverse Transform (Reconstruct)\n",
        "    X_compressed = pca.fit_transform(X)\n",
        "    X_reconstructed = pca.inverse_transform(X_compressed)\n",
        "\n",
        "    # Save the specific random image reconstruction\n",
        "    reconstructions.append(X_reconstructed[random_idx])\n",
        "\n",
        "    # Calculate Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(X[random_idx], X_reconstructed[random_idx])\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# --- PLOTTING ---\n",
        "\n",
        "# Plot 1: Scree Plot (Cumulative Variance)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(np.cumsum(explained_variance[:300])) # Zoomed in for clarity\n",
        "plt.title('Scree Plot (Elbow Method)')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Variance')\n",
        "\n",
        "# Plot 2: MSE vs k\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_values, mse_values, marker='o', color='red')\n",
        "plt.title('MSE vs k')\n",
        "plt.xlabel('k (Components)')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Grid of Reconstructions\n",
        "fig, axes = plt.subplots(1, len(k_values) + 1, figsize=(20, 5))\n",
        "axes[0].imshow(original_face.reshape(64, 64), cmap='gray')\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "for i, k in enumerate(k_values):\n",
        "    axes[i+1].imshow(reconstructions[i].reshape(64, 64), cmap='gray')\n",
        "    axes[i+1].set_title(f\"k={k}\")\n",
        "    axes[i+1].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GLTsycyNwuXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of PCA vs LDA\n",
        "\n",
        "## Dataset 1\n",
        "\n",
        "The dataset is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
        "\n",
        "[Dataset Link](https://archive.ics.uci.edu/dataset/109/wine)\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "- Perform Feature Engineeering for the above dataset.\n",
        "- Model 1: Train Logistic regression model for the above dataset.\n",
        "- Model 2: Perform PCA to the dataset, train logistic regression with the new datset\n",
        "- Model 3: Perform LDA to the dataset and use Logistic Regression for the above dataset.\n",
        "- Compare Train and Test accuracy for the Model 1, 2 and 3"
      ],
      "metadata": {
        "id": "0S0DKbNGpthq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fGlv0DEori_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. FEATURE SCALING (Crucial for PCA/LDA)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# --- APPROACH A: PCA + Logistic Regression ---\n",
        "# Compressing 13 features down to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "classifier_pca = LogisticRegression()\n",
        "classifier_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = classifier_pca.predict(X_test_pca)\n",
        "\n",
        "# --- APPROACH B: LDA + Logistic Regression ---\n",
        "# Compressing 13 features down to 2 components (Max is C-1, so 3-1=2)\n",
        "lda = LDA(n_components=2)\n",
        "X_train_lda = lda.fit_transform(X_train, y_train) # LDA needs y_train!\n",
        "X_test_lda = lda.transform(X_test)\n",
        "\n",
        "classifier_lda = LogisticRegression()\n",
        "classifier_lda.fit(X_train_lda, y_train)\n",
        "y_pred_lda = classifier_lda.predict(X_test_lda)\n",
        "\n",
        "# --- APPROACH C: Just Direct Logistic Regression ---\n",
        "classifier_reg = LogisticRegression()\n",
        "classifier_reg.fit(X_train, y_train)\n",
        "y_pred = classifier_reg.predict(X_test)\n",
        "\n",
        "# 4. Results Comparison\n",
        "print(f\"Accuracy with PCA: {accuracy_score(y_test, y_pred_pca):.2%}\")\n",
        "print(f\"Accuracy with LDA: {accuracy_score(y_test, y_pred_lda):.2%}\")\n",
        "print(f\"Accuracy without LDA/PCA: {accuracy_score(y_test, y_pred):.2%}\")"
      ]
    }
  ]
}